{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deep learning, representing model uncertainty is of crucial importance. However, standard deep learning tools for regression and classification do not capture model uncertainty.\n",
    "\n",
    "The use of dropout (and its variants) in NNs can be interpreted as a Bayesian approximation of a well known probabilistic model: the Gaussian process (GP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Dropout is used in many models in deep learning as a way to avoid over-fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We develop tools for representing model uncertainty of existing dropout NNs – extracting information that has been thrown away so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we give a complete theoretical treatment of the link between Gaussian processes and dropout, and develop\n",
    "the tools necessary to represent uncertainty in deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show that a neural network with arbitrary depth and non-linearities, with dropout applied before every weight\n",
    "layer, is mathematically equivalent to an approximation to the probabilistic deep Gaussian process.\n",
    "\n",
    "We show that the dropout objective, in effect, minimises the Kullback–Leibler divergence between an approximate distribution and the posterior of a deep Gaussian process (marginalised over its finite rank covariance function parameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout as a Bayesian Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout \n",
    "\n",
    "Minimisation objective of NN with $L_2$re gularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L_{dropout}=\\frac{1}{N}\\sum_{i=1}^{N}E(y_i,\\hat{y}_i)+\\lambda\\sum_i^L (||W_i||_2^2+||b_i||_2^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Gaussian\n",
    "\n",
    "covariance function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$K(x,y)=\\int p(\\omega)p(b)\\sigma(\\omega^Tx+b)\\sigma(\\omega^Ty+b)dwdb$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(y \\vert x, X, Y)=\\int p(y \\vert x,\\omega)p(\\omega \n",
    "\\vert X, Y)d\\omega$$\n",
    "$$p(y \\vert x,w)=\\mathcal(N)(y;\\hat{y}(x,\\omega),\\tau^{-1}I_D)$$\n",
    "$$\\hat{y}(x,\\omega=\\{W1,...,W_L\\})$$\n",
    "$$=\\sqrt{\\frac{1}{K_L}}W_L\\sigma(...\\sqrt{\\frac{1}{K_1}}W_2\\sigma(W_1x+m_1)...)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use $q(\\omega)$ to approximate $p(\\omega \\vert X, Y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$q(\\omega)$ is defined as:\n",
    "$$W_i=M_i\\cdot diag([z_{i,j}]_{j=1}^{K_i})$$\n",
    "$$z_{i,j}\\propto Bernoulli(p_i)$$ for i=1,...,L, j=1,...,$K_{i-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimisation Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$-\\int q(\\omega)log p(Y\\vert X,\\omega)d\\omega + KL(q(\\omega)||p(\\omega)))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first term can be rewritten as:\n",
    "\n",
    "$$-\\sum_{n=1}^{N}\\int q(\\omega)log p(y_n\\vert x_n,\\omega)d\\omega$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second term can be approximated by:\n",
    "\n",
    "$$\\sum_{i=1}^L (\\frac{p_il^2}{2}||M_i||_2^2+\\frac{l^2}{2}||m_i||_2^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given model precision $\\tau$ we scale the result by the constant $\\frac{1}{\\tau N}$ to obtain the objective:\n",
    "\n",
    "$$\\mathcal{L}_{GP-MC}\\propto \\frac{1}{N}\\sum_{n=1}^{N}\\frac{-log p(y_n|x_n,\\hat{\\omega}_n)}{\\tau}+\\sum_{i=1}^{L}(\\frac{p_il^2}{2\\tau N}||M_i||_2^2+\\frac{l^2}{2\\tau N}||m_i||_2^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting $$E(y_n,\\hat{y}(x_n,\\hat{\\omega}_n))=-log p(y_n \\vert x_n, \\hat{\\omega}_n)/\\tau$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining Model Uncertainty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our approximate predictive distribution is given by\n",
    "$$q(y^{*}\\vert x^{*})=\\int p(y^{*}\\vert x^{*},\\omega)q(\\omega)d\\omega$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moment-matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sample T sets of vectors of realisations from the Bernoulli distribution $\\{z_1^t,...,z_L^t\\}_{t=1}^T$\n",
    "\n",
    "$$E_{q(y^{*}\\vert x^{*})}\\approx \\frac{1}{T}\\sum_{t=1}^T \\hat(y)^{*}(x^{*},W_1^t,...,W_L^t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$E_{q(y^{*}\\vert x^{*})}((y^{*})^T y^{*})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\approx \\tau^{-1}I_D+\\frac{1}{T}\\sum_{t=1}^T \\hat{y}^{*}(x^{*}, W_1^t,...,W_L^t)^T\\hat{y}^{*}(x^{*},W_1^t,...,W_L^t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model predictive variance\n",
    "$$Var_{q(y^{*}\\vert x^{*})}(y^{*})$$\n",
    "$$\\approx \\tau^{-1}I_D+\\frac{1}{T}\\sum_{t=1}^{T}\\hat{y}^{*}(x^{*},W_1^t,...,W_L^t)^T\\hat{y}^{*}(x^{*},W_1^t,...,W_L^t)-E_{q(y^{*}\\vert x^{*})}(y^{*})^TE_{q(y^{*}\\vert x^{*})}(y^{*})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\tau=\\frac{pl^2}{2N\\lambda}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate predictive log-likelihood by Monte Carlo integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$log p(y^{*}\\vert x^{*},X,Y)$$\n",
    "$$\\approx logsumexp(-\\frac{1}{2}\\tau||y-\\hat{y}_t||^2)-logT-\\frac{1}{2}log2\\pi-\\frac{1}{2}log \\tau^{-1}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
